# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W-IYl8klwR9KrAJgpWBAhm6r-e5tn6q9

###Final Project

Name: 余采嬙 <br>
ID: 112356012
"""

!pip install datasets

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import create_optimizer
# from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report, confusion_matrix

from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from datasets import Dataset

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/Phishing_Email.csv'

# Option 1: Skipping bad lines (older pandas versions)
# raw_df = pd.read_csv(file_path, error_bad_lines=False)

# Option 1: Managing bad lines in newer pandas versions
raw_df = pd.read_csv(file_path) #on_bad_lines='skip'

# Option 2: Handling potential quoting issues
# raw_df = pd.read_csv(file_path, quoting=3)  # 3 corresponds to csv.QUOTE_NONE

raw_df[['Email Text', 'Email Type']]

"""### Data Preprocess"""

# 把 'empty' 換為 NaN
cleaned_df= raw_df.copy()
cleaned_df['Email Text'].replace('empty', np.nan, inplace=True)
cleaned_df
# 删除包含 NaN 的行
cleaned_df = cleaned_df.dropna(subset=['Email Text'])
# len(cleaned_df)

# 檢查空值數量
missing_values = cleaned_df.isna().sum()
print(missing_values) #確認無空值欄位

# Retain English characters and some punctuation
cleaned_df['Email Text'] = cleaned_df['Email Text'].str.replace('[^a-zA-Z\s.!?\'\"+$\-/]', '', regex=True)

# lower case
cleaned_df['Email Text'] = cleaned_df['Email Text'].str.lower()

# strip the blank
cleaned_df['Email Text'] = cleaned_df['Email Text'].str.replace('\s+', ' ', regex=True)
cleaned_df['Email Text'] = cleaned_df['Email Text'].str.strip()

print(cleaned_df['Email Text'].head())

"""Keep punctuation<br>
Sentiment analysis or fraud detection: Punctuation can sometimes provide clues to emotional intensity, such as multiple exclamation points that may express urgency or strong emotion, which is common in phishing emails.
Structural characteristics of text: Phishing emails may use specific punctuation patterns to attract attention or disguise formality. These patterns may help the model identify fraud.
"""

cleaned_df['Email Text'][200:400]

(cleaned_df.iloc[0]['Email Type'], cleaned_df.iloc[0]['Email Text'])

cleaned_df[["Email Text", "Email Type"]].head()

"""### Examine the class label imbalance

Let's look at the dataset imbalance:
"""

# 新增“Class”欄位，根據“Email Type”的值設置為1或0
cleaned_df['Class'] = cleaned_df['Email Type'].apply(lambda x: 0 if x == 'Phishing Email' else 1)

# 檢查結果
print(cleaned_df.head())

neg, pos = np.bincount(cleaned_df['Class'])
total = neg + pos
print('Examples:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, pos, 100 * pos / total))

#正向負向比例約為6:4=> 還算平衡

"""### LLM Model"""

from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased" #選定模型
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint) #載入模型

model.summary()

text = "This is a bad [MASK]."

# Install the datasets library
!pip install datasets

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) #載入字典

import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
# Find the location of [MASK] and extract its logits
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Pick the [MASK] candidates with the highest logits
# We negate the array before argsort to get the largest, not the smallest, logits
top_5_tokens = np.argsort(-mask_token_logits)[:8].tolist() #選擇機率最高的前5個候選字詞

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")

#此結果為一般尚未fine-tuned過的model（知識來自於Wikipedia）表現。

# # Install the datasets library
# !pip install datasets

# from datasets import load_dataset

# imdb_dataset = load_dataset("imdb")
# imdb_dataset

#希望模型學習新的資料集：
cleaned_df[["Email Text", "Email Type"]]

# Assuming tactics_df is original DataFrame
df_text_type = cleaned_df[["Email Text", "Email Type"]]

# Rename the columns
df = df_text_type.rename(columns={"Email Type": "label", "Email Text": "text"})

# Split the DataFrame into train and test sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Convert the DataFrames to Hugging Face Datasets
train_dataset = Dataset.from_pandas(train_df).remove_columns(['__index_level_0__'])
test_dataset = Dataset.from_pandas(test_df).remove_columns(['__index_level_0__'])

# Create a DatasetDict object
dataset_dict = DatasetDict({
    'train': train_dataset,
    'test': test_dataset
})


# Print the DatasetDict
print(dataset_dict)

sample = dataset_dict["train"].shuffle(seed=42).select(range(5))

for row in sample:
    print(f"\n'>>> Email Text: {row['text']}'")
    print(f"'>>> Email Label: {row['label']}'")

#只是演示，不是training:
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result #返回經 tokenizer 處理的结果，包括 input_ids 和（如果使用 fast tokenizer）word_ids


# Use batched=True to activate fast multithreading!
tokenized_datasets = dataset_dict.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets

tokenizer.model_max_length

chunk_size = 128

# Slicing produces a list of lists for each feature
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Mail {idx} length: {len(sample)}'")

#We can then concatenate all these examples with a simple dictionary comprehension:
concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")

chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")

"""Wrap all of the above logic in a single function that we can apply to our tokenized datasets:"""

def group_texts(examples):
    # Concatenate all texts
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # Compute length of concatenated texts
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the last chunk if it's smaller than chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    # Split by chunks of max_len
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # Create a new labels column
    result["labels"] = result["input_ids"].copy()
    return result


#Note that in the last step of group_texts() we create a new labels column which
#is a copy of the input_ids one. As we’ll see shortly, that’s because in masked
#language modeling the objective is to predict randomly masked tokens in the input
#batch, and by creating a labels column we provide the ground truth for our language
#model to learn from.

#Let’s now apply group_texts() to our tokenized datasets using our trusty Dataset.map() function:

lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets

# Calculate the lengths of the concatenated texts for each feature
concatenated_lengths = {k: len(concatenated_examples[k]) for k in concatenated_examples.keys()}

# Print the lengths of the concatenated texts
for feature, length in concatenated_lengths.items():
    print(f"Length of concatenated {feature}: {length}")

tokenizer.decode(lm_datasets["train"][1]["input_ids"])

"""Have to pass it the tokenizer and an mlm_probability argument that specifies what fraction of the tokens to mask. We’ll pick 15%, which is the amount used for BERT and a common choice in the literature:"""

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15) #mlm 做克漏字

samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")

#[MASK] token has been randomly inserted at various locations in our text.

train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset

tf_train_dataset = model.prepare_tf_dataset(
    downsampled_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = model.prepare_tf_dataset(
    downsampled_dataset["test"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)

#we set up our training hyperparameters and compile our model. Use the create_optimizer()
#function from the Transformers library, which gives us an AdamW optimizer with
#linear learning rate decay. We also use the model’s built-in loss, which is the
#default when no loss is specified as an argument to compile(), and we set the
#training precision to "mixed_float16".

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# model_name = model_checkpoint.split("/")[-1]
# callback = PushToHubCallback(
#     output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
# )

"""### Perplexity"""

import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")

model.fit(tf_train_dataset, validation_data=tf_eval_dataset) #callbacks=[callback]

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
#fine tune後的困惑度下降許多

"""### Using our fine-tuned model to do some downstream task

"""

#save model
folder_name = "model_0611_task"

model.save_pretrained(folder_name)

from transformers import pipeline

pretrained_model = TFAutoModelForMaskedLM.from_pretrained(folder_name)
# pretrained_tokenizer = AutoTokenizer.from_pretrained(folder_name)

mask_filler = pipeline(
    "fill-mask",
    model=pretrained_model, #huggingface-course/distilbert-base-uncased-finetuned-imdb #直接用剛剛fine-tuned完的model就好
    tokenizer=tokenizer,
)
#克漏字測驗

orginal_mask_filler = pipeline('fill-mask', model='distilbert-base-uncased')
text = "This is a [MASK] Email."
preds = orginal_mask_filler(text)

for pred in preds:
    print(f">>> {pred}")

#original Model results have more common terms

#We can then feed the pipeline our sample text of “This is a great [MASK]”
#and see what the top 5 predictions are:

preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred}")

# for pred in preds:
#     print(f">>> {pred['sequence']}")
#克漏字結果有些許改變



pipe = pipeline("text-classification", model='distilbert-base-uncased')
pipe("software at incredibly low prices ( 86 % lower )")
# pipe("They haven't found a hobbit that can code, so mortal humans have to suffice")

#use out of sample data to test the original model classification
#the original model cannot classify the phishing model

# pipe= pipeline("text-classification", model=pretrained_model,)
# pipe("software at incredibly low prices ( 86 % lower )")

"""## Use Other Model(SVM)"""

# 分X,Y
X = cleaned_df['Email Text']
y = cleaned_df['Class']


sampling_fraction = 0.1

# random sampling
np.random.seed(42)
sample_indices = np.random.choice(len(X), size=int(len(X) * sampling_fraction), replace=False)
X_sampled = X.iloc[sample_indices]
y_sampled = y.iloc[sample_indices]

# 文本feature extract
vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(X_sampled)


X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_sampled, test_size=0.2, random_state=42)

svm_model = SVC(probability=True)
svm_model.fit(X_train, y_train)

# 预测
y_pred = svm_model.predict(X_test)
y_probs = svm_model.predict_proba(X_test)[:, 1]

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'SVM Accuracy: {accuracy:.4f}')

# Precision
precision = precision_score(y_test, y_pred)
print(f'Precision: {precision:.4f}')

# Recall
recall = recall_score(y_test, y_pred)
print(f'Recall: {recall:.4f}')

# F1-score
f1 = f1_score(y_test, y_pred)
print(f'F1 Score: {f1:.4f}')

# ROC-AUC
roc_auc = roc_auc_score(y_test, y_probs)
print(f'ROC-AUC: {roc_auc:.4f}')

# print Classification Report:
print('Classification Report:')
print(classification_report(y_test, y_pred))

# confusion_matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# plot confusion_matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ROC curve
fpr, tpr, _ = roc_curve(y_test, y_probs)

# plot ROC
plt.figure()
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()



"""## Use Bert LM(distilbert-base-uncased) and do the downstream task(classification) directly"""

# 匯入 transformers 和 datasets 庫
!pip install transformers datasets scikit-learn

from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf
from datasets import Dataset
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# 選定模型和載入字典
model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# 將文本轉換為 tokens
def tokenize_function(examples):
    return tokenizer(examples['Email Text'], padding='max_length', truncation=True)

# 將 DataFrame 轉換為 datasets 格式
dataset = Dataset.from_pandas(cleaned_df[['Email Text', 'Class']])

# Tokenize the dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# downstream資料集
train_size = 10_000
test_size = int(0.1 * train_size)
downsampled_dataset = tokenized_datasets.train_test_split(train_size=train_size, test_size=test_size, seed=42)

# 準備資料集
train_dataset = downsampled_dataset["train"]
eval_dataset = downsampled_dataset["test"]

# 將資料集轉換為 TensorFlow 格式
def dataset_to_tf_dataset(dataset, batch_size):
    def gen():
        for i in range(len(dataset)):
            yield {
                "input_ids": dataset[i]["input_ids"],
                "attention_mask": dataset[i]["attention_mask"],
                "labels": float(dataset[i]["Class"])  # 確保 labels 是 float
            }

    output_signature = {
        "input_ids": tf.TensorSpec(shape=(None,), dtype=tf.int32),
        "attention_mask": tf.TensorSpec(shape=(None,), dtype=tf.int32),
        "labels": tf.TensorSpec(shape=(), dtype=tf.float32)  # 修改這裡的 dtype 為 float32
    }

    tf_dataset = tf.data.Dataset.from_generator(gen, output_signature=output_signature)
    tf_dataset = tf_dataset.batch(batch_size)
    return tf_dataset

train_tf_dataset = dataset_to_tf_dataset(train_dataset, batch_size=32)
eval_tf_dataset = dataset_to_tf_dataset(eval_dataset, batch_size=32)

# 載入預訓練模型
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1)  # 修改這裡的 num_labels 為 1

model.summary()

# 定義模型訓練時的回調函數
class SavePretrainedCallback(tf.keras.callbacks.Callback):
    def __init__(self, output_dir, monitor='val_loss', mode='min', save_best_only=True):
        super(SavePretrainedCallback, self).__init__()
        self.output_dir = output_dir
        self.monitor = monitor
        self.mode = mode
        self.save_best_only = save_best_only
        if self.mode == 'min':
            self.best = np.Inf
        else:
            self.best = -np.Inf

    def on_epoch_end(self, epoch, logs=None):
        current = logs.get(self.monitor)
        if current is not None:
            if (self.mode == 'min' and current < self.best) or (self.mode == 'max' and current > self.best):
                self.best = current
                if self.save_best_only:
                    print(f'Saving model at epoch {epoch + 1} with {self.monitor} = {current}')
                    self.model.save_pretrained(self.output_dir)
                else:
                    self.model.save_pretrained(f"{self.output_dir}/checkpoint-{epoch + 1}")

# Early stopping callback
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,  # Stop if no improvement for 5 epochs
    restore_best_weights=True  # Restore the best weights after stopping
)

# 編譯模型
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = [tf.metrics.BinaryAccuracy()]

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

# 訓練模型
history = model.fit(
    train_tf_dataset,
    validation_data=eval_tf_dataset,
    epochs=25,
    callbacks=[early_stopping_callback, SavePretrainedCallback(output_dir="model_0611_test", monitor='val_loss', mode='min')]
)

# 評估模型
eval_labels = []
eval_preds = []

for batch in eval_tf_dataset:
    inputs = {key: value for key, value in batch.items() if key != "labels"}
    labels = batch["labels"]
    eval_labels.extend(labels.numpy())
    preds = model.predict(inputs)
    eval_preds.extend(tf.nn.sigmoid(preds.logits).numpy().flatten())

# 二元分類情況下的閾值設置為0.5
eval_preds = [1 if p > 0.5 else 0 for p in eval_preds]

# 計算評估指標
accuracy = accuracy_score(eval_labels, eval_preds)
conf_matrix = confusion_matrix(eval_labels, eval_preds)
roc_auc = roc_auc_score(eval_labels, eval_preds)
fpr, tpr, _ = roc_curve(eval_labels, eval_preds)

print(f"Accuracy: {accuracy}")
print(f"Confusion Matrix:\n{conf_matrix}")
print(f"ROC AUC: {roc_auc}")

# 繪製ROC曲線
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# 計算Precision-Recall曲線
!pip install seaborn

# 匯入必要的庫
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import precision_recall_curve, roc_auc_score, roc_curve, accuracy_score, confusion_matrix, ConfusionMatrixDisplay


# 計算Precision-Recall曲線
precision, recall, _ = precision_recall_curve(eval_labels, eval_preds)

# # 繪製Precision-Recall曲線
# plt.figure()
# plt.plot(recall, precision, color='blue', lw=2)
# plt.xlabel('Recall')
# plt.ylabel('Precision')
# plt.title('Precision-Recall Curve')
# plt.show()

# 繪製混淆矩陣圖
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

